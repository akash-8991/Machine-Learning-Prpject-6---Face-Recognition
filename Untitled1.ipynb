{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "from skimage import io,transform\n",
    "from PIL import Image \n",
    "import os\n",
    "# from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms, utils,models\n",
    "device = torch.device(\"cuda:0:\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "directory_name =\"./pins-face-recognition/\"\n",
    "\n",
    "\n",
    "# list all directory of celebrity \n",
    "all_directory_path = [x[0] for x in os.walk(directory_name)][1:]\n",
    "\n",
    "#vocabulary creation\n",
    "#create necessary vocabulary for later use\n",
    "dirs = sorted(os.listdir(directory_name))\n",
    "name_to_classid = {d:i for i,d in enumerate(dirs)}\n",
    "classid_to_name = {v:k for k,v in name_to_classid.items()}\n",
    "num_classes = len(name_to_classid)\n",
    "print(\"number of classes: \"+str(num_classes))\n",
    "\n",
    "# read all directories\n",
    "img_paths = {c:[directory + \"/\" + img for img in sorted(os.listdir(directory_name+directory))] \n",
    "             for directory,c in name_to_classid.items()}\n",
    "# retrieve all images\n",
    "all_images_path = []\n",
    "for img_list in img_paths.values():\n",
    "    all_images_path += img_list\n",
    "# map to integers\n",
    "path_to_id = {v:k for k,v in enumerate(all_images_path)}\n",
    "id_to_path = {v:k for k,v in path_to_id.items()}\n",
    "\n",
    "#get total images\n",
    "def totalImageCount(directory):\n",
    "    total_images=0\n",
    "    for i in range(len(all_directory_path)):\n",
    "        total_images += len(list(os.walk(all_directory_path[i]))[0][2])\n",
    "    return total_images\n",
    "print(\"total directory is \",len(all_directory_path))\n",
    "print(\"total images are \" ,totalImageCount(all_directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show sample image in all celebrity directory\n",
    "def show_sample_images(n_samples=5,directory_name=directory_name):\n",
    "    directory=os.listdir(directory_name)\n",
    "    for each in directory[:n_samples]:\n",
    "        plt.figure()\n",
    "        currentFolder = directory_name+'/' + each\n",
    "        for i, file in enumerate(os.listdir(currentFolder)[0:n_samples]):\n",
    "            fullpath = currentFolder+\"/\"+ file\n",
    "            i=i+1\n",
    "            img=io.imread(os.path.join(fullpath))\n",
    "            plt.subplot(1,n_samples,i)\n",
    "            plt.imshow(img)\n",
    "            plt.suptitle(each)\n",
    "show_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show sample images from single celebrity directory\n",
    "def show_sample_from_directory(directory_index):\n",
    "    i=0\n",
    "    print(classid_to_name[directory_index],\"images\")\n",
    "    _ ,fig = plt.subplots(4, 4, figsize=(12,12))\n",
    "    dir_images=img_paths[directory_index]\n",
    "    \n",
    "    fig = fig.flatten()\n",
    "    for f in fig:\n",
    "        image_path=os.path.join(directory_name+\"/\"+str(dir_images[i]))\n",
    "        f.imshow(io.imread(image_path))\n",
    "        i=i+1\n",
    "show_sample_from_directory(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train,dataset_validation =train_test_split(dataset,test_size=0.2)\n",
    "\n",
    "#check the distribution of classes \n",
    "dataset_validation.groupby(\"class\").count().head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training set \",len(dataset_train))\n",
    "print(\"number of development set\",len(dataset_validation))\n",
    "\n",
    "#necessary vocab(for developer understanding who is a  yo yo ganjini )\n",
    "classid_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "class PinterestDataset(Dataset):\n",
    "    '''\n",
    "    Dataframe : pandas dataframe which contains celebrity class and corresponding image path\n",
    "    key note:\n",
    "    we have to choose positive and negative sample for each targets for dev and training set\n",
    "    '''\n",
    "    def __init__(self,dataframe,transform):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "#         self.train = isTrain\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #get image name\n",
    "        image_name= str(self.data.iloc[idx,0])\n",
    "        #get image array\n",
    "        anchor_image = io.imread(image_name)\n",
    "        #get class of the image\n",
    "        class_value = self.data.iloc[idx,1]\n",
    "        #do positive sampling\n",
    "        #get all data relative to the corresponding image class\n",
    "        get_all_positive = self.data[self.data[\"class\"]==class_value]\n",
    "        #remove the particular image data from queried data\n",
    "        get_all_positive = get_all_positive[get_all_positive[\"images\"]!=image_name]\n",
    "        #choose random data from that filtered dataset\n",
    "        positive_frame =get_all_positive.sample(n=1)\n",
    "        positive_image = io.imread(str(positive_frame.iloc[0,0]))\n",
    "        #choose negative data by taking random sampling except that particular class\n",
    "        get_all_negative = self.data[self.data[\"class\"]!=class_value]\n",
    "        negative_frame = get_all_negative.sample(n=1)\n",
    "        negative_image = io.imread(str(negative_frame.iloc[0,0]))\n",
    "        \n",
    "#return_array = [anchor_image,positive_image,negative_image]\n",
    "        \n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "            positive_image = self.transform(positive_image)\n",
    "            negative_image = self.transform(negative_image)\n",
    "       # return torch.tensor([anchor_image,positive_image,negative_image])\n",
    "        return {\n",
    "            \"anchor\":anchor_image,\n",
    "            \"positive_image\":positive_image,\n",
    "            \"negative_image\":negative_image\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation Custom Loader Class\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "#         assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image = sample\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "#         print(h,w)\n",
    "#         if isinstance(self.output_size, int):\n",
    "#             if h > w:\n",
    "#                 new_h, new_w = self.output_size * h / w, self.output_size\n",
    "#             else:\n",
    "#                 new_h, new_w = self.output_size, self.output_size * w / h\n",
    "#         else:\n",
    "#             new_h, new_w = self.output_size\n",
    "        new_h = self.output_size\n",
    "        new_w = self.output_size\n",
    "#         new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w,3))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image = sample\n",
    "#         print(type(torch.from_numpy(image)))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "#         print(image.shape)\n",
    "     \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self,mean,std):\n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    "    def __call__(self,sample):\n",
    "        image = sample\n",
    "        normalized=  (image -self.mean) / self.std\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset_train = PinterestDataset(dataset_train,\n",
    "                                        transform=transforms.Compose([Rescale(224),ToTensor(),Normalize(0.5,0.5)\n",
    "                                                          ]))\n",
    "transformed_dataset_validation = PinterestDataset(dataset_validation,\n",
    "                                        transform=transforms.Compose([Rescale(224),ToTensor(),Normalize(0.5,0.5)\n",
    "                                                          ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8000):\n",
    "#     a=transformed_dataset_train[i]\n",
    "\n",
    "# plt.imshow(a[\"anchor\"][0])\n",
    "\n",
    "train_dataloader=DataLoader(transformed_dataset_train,batch_size=32,shuffle=True)\n",
    "validation_dataloader=DataLoader(transformed_dataset_validation,batch_size=32,shuffle=True)\n",
    "\n",
    "def im_convert(tensor):\n",
    " \n",
    "#     image = tensor.numpy()\n",
    "    image = tensor.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "# plt.imshow((sample[\"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i= 898\n",
    "sample_anchor= im_convert(transformed_dataset_train[i][\"anchor\"])\n",
    "sample_pos= im_convert(transformed_dataset_train[i][\"positive_image\"])\n",
    "sample_neg= im_convert(transformed_dataset_train[i][\"negative_image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset_train[i][\"anchor\"].shape\n",
    "\n",
    "plt.imshow(sample_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_neg)\n",
    "\n",
    "# for i,j in enumerate(train_dataloader):\n",
    "#     print(j[\"anchor\"].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Sampling batch\n",
    "\n",
    "test=DataLoader(transformed_dataset_train,batch_size=5,shuffle=True)\n",
    "test_batch =iter(test).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_convert(tensor):\n",
    "    image = tensor.cpu().clone().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "# plt.imshow((sample[\"image\"]))\n",
    "def visualize_samples(dictionary):\n",
    "#     print(n_samples)\n",
    "    for key,data in enumerate(dictionary):\n",
    "        plt.figure()\n",
    "        plt.suptitle(data)\n",
    "        for i in range((dictionary[data][:5].shape[0])):\n",
    "            j=i\n",
    "            i=i+1\n",
    "            plt.subplot(1,5,i)            \n",
    "            plt.imshow(img_convert(dictionary[data][j]))\n",
    "\n",
    "visualize_samples(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# resnet= resnet.double()\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = resnet.fc.in_features\n",
    "\n",
    "last_layer = nn.Linear(n_inputs, 128)\n",
    "resnet.fc = last_layer\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Creation\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self,pretrained_net):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.resnet = pretrained_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.resnet(x)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    '''\n",
    "    input\n",
    "    embedding net : ConvNet which takes torch.tensor input\n",
    "     run parallel convnet for each batch\n",
    "    '''\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        output3 = self.embedding_net(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "embedding_net = EmbeddingNet(resnet)\n",
    "model = TripletNet(embedding_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Triple Loss Class\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        margin : (float) \n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = ((anchor - positive)**2).sum(axis=1)  \n",
    "        distance_negative = ((anchor - negative)**2).sum(axis=1) \n",
    "        losses = max(0,distance_positive - distance_negative + self.margin)\n",
    "     \n",
    "        return losses.mean()\n",
    "\n",
    "margin = 0.5\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# running_loss_history = []\n",
    "# val_running_loss_history=[]\n",
    "# losses=[]\n",
    "# total_loss=0\n",
    "# for e in range(epochs):\n",
    "#     running_loss=0.0\n",
    "#     val_running_loss=0.0\n",
    "#     for i,batched_data in enumerate(train_dataloader):\n",
    "#         print(i)\n",
    "#         input_anchor= batched_data[\"anchor\"].to(device)\n",
    "#         input_positive = batched_data[\"positive_image\"].to(device)\n",
    "#         input_negative = batched_data[\"negative_image\"].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_anchor.float(),input_positive.float(),input_negative.float())\n",
    "#         if type(outputs) not in (tuple, list):\n",
    "#             outputs = (outputs,)\n",
    "#         loss_inputs = outputs\n",
    "#         loss_outputs = loss_fn(*loss_inputs)\n",
    "#         loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "#         losses.append(loss.item())\n",
    "#         total_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# #         loss = loss_fn(input_anchor.float(),input_positive.float(),input_negative.float())\n",
    "# #         optimizer.zero_grad()\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "#         running_loss+=loss.item()\n",
    "#         print(loss.item())\n",
    "#     else:\n",
    "#         pass\n",
    "# #         with torch.no_grad():\n",
    "# #             for i,batched_val in enumerate(validation_dataloader):\n",
    "# #                 val_input_anchor = batched_val[\"anchor\"].to(device)\n",
    "# #                 val_input_positive = batched_val[\"positive_image\"].to(device)\n",
    "# #                 val_input_negative = batched_val[\"negative_image\"].to(device)\n",
    "               \n",
    "# #                 val_outputs = model(val_input_anchor.float(),val_input_positive.float(),val_input_negative.float())\n",
    "# #                 val_loss = loss_fn(val_input_anchor.float(),val_input_positive.float(),val_input_negative.float())\n",
    "# #                 val_running_loss+=val_loss.item()\n",
    "# # #         print(val_loss.item())\n",
    "    \n",
    "# #   epoch_loss\n",
    "# #     epoch_loss = running_loss/len(train_dataloader.dataset)\n",
    "  \n",
    "# #     running_loss_history.append(epoch_loss)\n",
    "    \n",
    "# #     val_epoch_loss = val_running_loss/len(validation_dataloader.dataset)\n",
    "# #     val_running_loss_history.append(val_epoch_loss)\n",
    "# #     print('epoch :', (e+1))\n",
    "# #     print('training loss: {:.4f}'.format(epoch_loss))\n",
    "# #     print('validation loss: {:.4f}'.format(val_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, loss_fn, optimizer,printer):\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "    for batch_idx, batched_data in enumerate(train_loader):\n",
    "        \n",
    "        input_anchor= batched_data[\"anchor\"].to(device).float()\n",
    "        input_positive = batched_data[\"positive_image\"].to(device).float()\n",
    "        input_negative = batched_data[\"negative_image\"].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_anchor,input_positive,input_negative)\n",
    "\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "        loss_inputs = outputs\n",
    "        loss_outputs = loss_fn(*loss_inputs)\n",
    "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch_idx % printer == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(batched_data[\"anchor\"][0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            \n",
    "            print(message)\n",
    "            losses = []\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss\n",
    "\n",
    "# train_one_epoch(train_dataloader,model,loss_fn,optimizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(val_loader, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for batch_idx,val_batched_data in enumerate(val_loader):\n",
    "            print(batch_idx)\n",
    "            val_anchor=val_batched_data[\"anchor\"].to(device).float()\n",
    "            val_positive = val_batched_data[\"positive_image\"].to(device).float()\n",
    "            val_negative =val_batched_data[\"negative_image\"].to(device).float()\n",
    "            outputs = model(val_anchor,val_positive,val_negative)\n",
    "\n",
    "            if type(outputs) not in (tuple, list):\n",
    "                outputs = (outputs,)\n",
    "            loss_inputs = outputs\n",
    "            loss_outputs = loss_fn(*loss_inputs)\n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            val_loss += loss.item()\n",
    "\n",
    "           \n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# test_one_epoch(validation_dataloader,model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, printer,start_epoch=0):\n",
    "   \n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        scheduler.step()\n",
    "\n",
    "        # Train stage\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer, printer)\n",
    "\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        \n",
    "        val_loss = test_one_epoch(val_loader, model, loss_fn)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
    "                                                                                 val_loss)\n",
    "       \n",
    "        print(message)\n",
    "\n",
    "train(train_dataloader,validation_dataloader,model,loss_fn,optimizer,scheduler,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved_model v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring Validation\n",
    "\n",
    "sample_batch =iter(validation_dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_anchor = sample_batch[\"anchor\"].float()\n",
    "sample_positive = sample_batch[\"positive_image\"].float()\n",
    "sample_negative = sample_batch[\"negative_image\"].float()\n",
    "\n",
    "#predict for validation batch\n",
    "#get embedding for anchor\n",
    "sample_anchor_emb = model.get_embedding(sample_anchor)\n",
    "\n",
    "#get embedding for positive \n",
    "sample_positive_emb = model.get_embedding(sample_positive)\n",
    "\n",
    "#get embedding for negative \n",
    "sample_negative_emb=model.get_embedding(sample_negative)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "i=31\n",
    "#distance between anchor and negative\n",
    "cosine_similarity(sample_anchor_emb[i].detach().numpy(),sample_negative_emb[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between anchor and positive\n",
    "cosine_similarity(sample_anchor_emb[i].detach().numpy(),sample_positive_emb[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pos=[]\n",
    "dist_neg =[]\n",
    "for i in range(31):\n",
    "    dist_pos.append(cosine_similarity(sample_anchor_emb[i].detach().numpy(),sample_positive_emb[i].detach().numpy()))\n",
    "    dist_neg.append( cosine_similarity(sample_anchor_emb[i].detach().numpy(),sample_negative_emb[i].detach().numpy()) )\n",
    "df = pd.DataFrame()\n",
    "df[\"dist between positive and anchor\"]= dist_pos\n",
    "df[\"dist between negative and anchor\"]= dist_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single batch validation\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(image_array,n_row=1,n_col=4,vector_a=None,vector_b=None):\n",
    "\n",
    "    i=0\n",
    "    _ ,fig = plt.subplots(n_row, n_col, figsize=(12,12))\n",
    "#   print(fig)\n",
    "    \n",
    "    fig = fig.flatten()\n",
    "#   print(fig)\n",
    "    for f in fig:\n",
    "#     print(i,f)\n",
    "        image= image_array[i]\n",
    "        f.imshow(img_convert(image))\n",
    "        if vector_a is not None:\n",
    "            distance = cosine_similarity(vector_a[i].detach().numpy(),vector_b[i].detach().numpy())\n",
    "            f.title.set_text(str(distance))\n",
    "        i=i+1\n",
    "    \n",
    "#     f.title(str(aseth_value))\n",
    "       \n",
    "print(\"validation sample Anchor images are\")\n",
    "show_grid(sample_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample validation positive images \")\n",
    "show_grid(sample_positive,vector_a=sample_anchor_emb,vector_b=sample_positive_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample validation negative images are\")\n",
    "show_grid(sample_negative,vector_a=sample_anchor_emb,vector_b=sample_negative_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
